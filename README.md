<p align="center">
  <img src="app/static/avatar.png" alt="Jasper Spencer - Avatar" width="140" />
</p>

<h1 align="center">Jasper Spencer â€“ Assistente IA Local</h1>

<p align="center">
  Assistente IA Local â€¢ FastAPI + Ollama â€¢ UI estilo ChatGPT
</p>

<p align="center">
  <em>Conversas em portuguÃªs, IA rodando na sua mÃ¡quina, com memÃ³ria, upload de arquivos e interface moderna.</em>
</p>

---

## âœ¨ VisÃ£o Geral

O **Jasper Spencer** Ã© um assistente de IA local, pensado como um case tÃ©cnico completo:

- Backend em **FastAPI**
- Modelo de linguagem rodando via **Ollama** na sua mÃ¡quina
- **Interface web** estilo ChatGPT/Gemini (HTML + CSS + JS)
- **MemÃ³ria conversacional** persistente em SQLite
- Upload de **PDF/DOCX** com resumo automÃ¡tico
- Modo **voz** (fala e ouve) usando APIs do navegador

Tudo isso rodando 100% **local** â€“ ideal para estudos, demonstraÃ§Ãµes tÃ©cnicas e uso offline.

---

## ğŸ–¼ Screenshot

> Exemplo da interface do Jasper em execuÃ§Ã£o:

<p align="center">
  <img src="docs/jasper-screen.png" alt="Screenshot Jasper Spencer" width="850" />
</p>

> ğŸ’¡ Dica: salve sua imagem em `docs/jasper-screen.png` (ou ajuste o caminho acima).

---

## ğŸš€ Principais Funcionalidades

- ğŸ’¬ **Chat em linguagem natural** em portuguÃªs
- ğŸ§  **MemÃ³ria conversacional** (SQLite) â€“ o Jasper lembra do contexto recente
- ğŸ§¾ **Upload de arquivos PDF/DOC/DOCX** e geraÃ§Ã£o de resumos
- ğŸ”Š **Modo voz**:  
  - Fala â†’ texto â†’ Jasper â†’ resposta  
  - Resposta â†’ voz (text-to-speech)
- ğŸ¨ **UI moderna**:
  - Tema dark com glassmorphism
  - Sidebar estilo ChatGPT
  - Avatar animado do Jasper
  - SugestÃµes iniciais de prompts
  - AnimaÃ§Ã£o de digitaÃ§Ã£o (â€œJasper estÃ¡ digitando...â€ com 3 bolinhas)
- ğŸ—‘ **Limpar chat** com um clique (reseta a conversa da interface)
- âš™ï¸ ConfiguraÃ§Ã£o via `.env` (modelo do Ollama, temperatura, system prompt)

---

## ğŸ§± Stack TecnolÃ³gica

**Backend**

- [FastAPI](https://fastapi.tiangolo.com/)
- [Uvicorn](https://www.uvicorn.org/)
- [Ollama](https://ollama.com/) (LLM local â€“ ex.: `llama3.1`)
- `requests`
- `python-dotenv`
- `sqlite3`
- `PyPDF2` (PDF)
- `python-docx` (DOC/DOCX)

**Frontend**

- HTML5
- CSS3 (tema dark/light, glassmorphism, animaÃ§Ãµes)
- JavaScript (Axios, Web Speech API, animaÃ§Ã£o de digitaÃ§Ã£o)

---

## ğŸ—‚ Estrutura do Projeto

```bash
ia-chat-strands-ollama/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py          # FastAPI: rotas /, /chat, /upload, static, CORS
â”‚   â”œâ”€â”€ agent.py         # LÃ³gica do agente Jasper (Ollama + memÃ³ria)
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ database.py  # init_db, save_message, load_memory (SQLite)
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ index.html   # Interface do chat
â”‚       â”œâ”€â”€ style.css    # Estilos (tema dark, sidebar, bolhas, etc.)
â”‚       â”œâ”€â”€ app.js       # LÃ³gica do front (chat, voz, upload, animaÃ§Ãµes)
â”‚       â””â”€â”€ avatar.png   # Avatar do Jasper
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ jasper-screen.png   # Screenshot da interface
â”œâ”€â”€ .env                    # ConfiguraÃ§Ãµes do modelo e do agente (criado por vocÃª)
â”œâ”€â”€ .env.example            # Exemplo de .env
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”œâ”€â”€ INSTALACAO.md           # (Opcional) Guia passo a passo
â””â”€â”€ README.md               # Este arquivo
âš™ï¸ Como Rodar o Projeto Localmente
1. PrÃ©-requisitos
Python 3.10+

Ollama instalado
âœ https://ollama.com/

2. Clonar o repositÃ³rio
bash
Copiar cÃ³digo
git clone https://github.com/SEU_USUARIO/ia-chat-strands-ollama.git
cd ia-chat-strands-ollama
3. Criar e ativar o ambiente virtual
Windows (PowerShell):

bash
Copiar cÃ³digo
python -m venv .venv
.\.venv\Scripts\Activate.ps1
Windows (cmd):

bash
Copiar cÃ³digo
python -m venv .venv
.\.venv\Scripts\activate
Linux / macOS:

bash
Copiar cÃ³digo
python3 -m venv .venv
source .venv/bin/activate
4. Instalar as dependÃªncias
bash
Copiar cÃ³digo
pip install -r requirements.txt
Se necessÃ¡rio, instale tambÃ©m:

bash
Copiar cÃ³digo
pip install PyPDF2 python-docx
5. Configurar o Ollama
Baixar o modelo (ex.: llama3.1):

bash
Copiar cÃ³digo
ollama pull llama3.1
Rodar o servidor do Ollama (em outro terminal):

bash
Copiar cÃ³digo
ollama serve
Deixe esse terminal aberto â€“ Ã© o â€œmotorâ€ da IA.

6. Configurar o .env
Crie um arquivo .env na raiz do projeto (ao lado do requirements.txt) com, por exemplo:

ini
Copiar cÃ³digo
AGENT_MODEL="llama3.1"
AGENT_TEMPERATURE="0.2"
AGENT_SYSTEM_PROMPT="VocÃª Ã© Jasper Spencer â€“ Assistente IA Local. Responda SEMPRE de forma natural, clara e humana.

REGRAS IMPORTANTES E OBRIGATÃ“RIAS:
1. NUNCA mostre, mencione, descreva ou revele ferramentas internas.
2. NUNCA exiba JSON, tool_calls, cÃ³digo, estruturas internas ou metadados.
3. Sempre que houver nÃºmeros, contas ou expressÃµes matemÃ¡ticas, faÃ§a o cÃ¡lculo internamente e responda apenas com o resultado final.
4. A resposta ao usuÃ¡rio deve ser SOMENTE o resultado final â€” humano, natural e direto.
5. Para cÃ¡lculos, responda apenas: 'A resposta Ã© X.' sem mostrar passos.
6. Para perguntas comuns, responda em 1â€“2 frases claras.
7. Se o modelo tentar responder em JSON ou formato tÃ©cnico, ignore isso e devolva apenas texto natural em portuguÃªs."
7. Rodar o backend (FastAPI)
Com a venv ativada:

bash
Copiar cÃ³digo
uvicorn app.main:app --reload --host 127.0.0.1 --port 8001
Se tudo der certo, verÃ¡ algo como:

text
Copiar cÃ³digo
Uvicorn running on http://127.0.0.1:8001
Application startup complete.
8. Abrir o Jasper no navegador
Acesse:

text
Copiar cÃ³digo
http://127.0.0.1:8001/
Pronto! ğŸ‰
VocÃª jÃ¡ pode conversar com o Jasper Spencer, enviar arquivos, usar o modo de voz e testar toda a interface.

ğŸ”Œ Endpoints
AlÃ©m da interface web, vocÃª pode consumir a API diretamente.

POST /chat
Request

json
Copiar cÃ³digo
{
  "message": "Me explique o que Ã© FastAPI em poucas palavras."
}
Response

json
Copiar cÃ³digo
{
  "response": "FastAPI Ã© um framework web em Python focado em alta performance e construÃ§Ã£o rÃ¡pida de APIs."
}
POST /upload
Multipart (PDF/DOC/DOCX)

Retorna um JSON com o resumo do conteÃºdo do arquivo.

ğŸ§  MemÃ³ria Conversacional
O mÃ³dulo app/memory/database.py usa SQLite para salvar:

session_id

role (user ou assistant)

content

created_at

O agent.py:

Carrega as Ãºltimas mensagens da sessÃ£o

Monta o contexto com:

system prompt

histÃ³rico

mensagem atual

Comprime o histÃ³rico se ficar muito grande

Envia tudo ao modelo do Ollama

Salva as mensagens novamente no banco

Assim o Jasper lembra do que foi dito recentemente e responde de forma mais coerente.

ğŸ—º Roadmap / Ideias Futuras
HistÃ³rico real de conversas na sidebar (multi-sessÃ£o)

AutenticaÃ§Ã£o simples para uso em rede local

Dockerfile para rodar tudo containerizado

IntegraÃ§Ã£o com outros modelos (OpenAI, Gemini) como fallback

Painel de configuraÃ§Ãµes avanÃ§adas direto da interface

ğŸ‘©â€ğŸ’» Autoria
Projeto desenvolvido por [SEU NOME / Dev.Pamela MS], como case tÃ©cnico de Assistente IA Local com Ollama + FastAPI, explorando:

Backend em Python

IntegraÃ§Ã£o com LLM local

Frontend moderno e interativo

MemÃ³ria persistente e manipulaÃ§Ã£o de arquivos

Se este projeto ajudou vocÃª de alguma forma, considere deixar uma â­ no repositÃ³rio. ğŸ’œ
