# ğŸ¤– Jasper Spencer â€“ Assistente IA Local (FastAPI + Ollama)

Este projeto Ã© um **case tÃ©cnico** que demonstra a criaÃ§Ã£o de um **Assistente de IA local** chamado **â€œJasper Spencerâ€**, com:

- **Backend em FastAPI** (API REST)
- **Modelo de linguagem local via Ollama**
- **Interface web moderna estilo ChatGPT/Gemini (HTML + CSS + JS)**
- **MemÃ³ria conversacional persistente com SQLite**
- **Upload de PDF/DOCX com resumo automÃ¡tico**
- **Modo de voz (fala e ouve) via Web Speech API (frontend)**
- **ConfiguraÃ§Ã£o via `.env` para modelo, temperatura e instruÃ§Ãµes do agente**

O objetivo central Ã© expor um endpoint `/chat` que permita conversar com o agente de IA e, adicionalmente, oferecer recursos extras (memÃ³ria, arquivos, voz, UI avanÃ§ada), atendendo e indo alÃ©m do case proposto pela empresa. :contentReference[oaicite:0]{index=0}  

---

## ğŸ§¾ Case original (resumo)

O case solicitado Ã©:

> Desenvolver uma API de Chat simples que se conecta a um Agente de IA, configurado para utilizar uma **tool de cÃ¡lculo matemÃ¡tico** para operaÃ§Ãµes numÃ©ricas, rodando localmente com **Ollama** como LLM, e expor um endpoint `POST /chat` que recebe `message` e retorna `response`. :contentReference[oaicite:1]{index=1}  

Este projeto implementa:

- API FastAPI com `/chat` seguindo o contrato `{"message": "..."} -> {"response": "..."}`  
- ConfiguraÃ§Ã£o via `.env` para LLM, sistema do agente e parÃ¢metros de execuÃ§Ã£o  
- ExecuÃ§Ã£o local com **Ollama** e modelo configurÃ¡vel  
- LÃ³gica de agente dedicada (`app/agent.py`)  
- Ferramentas adicionais alÃ©m do mÃ­nimo exigido:
  - MemÃ³ria conversacional persistente
  - Interface web em HTML/CSS/JS
  - Upload e leitura de documentos (PDF/DOCX)
  - Voice-to-text e text-to-speech no front

---

## ğŸ›  Tecnologias Utilizadas

| Tecnologia        | Uso                                                                 |
|------------------|---------------------------------------------------------------------|
| **FastAPI**      | Framework web para a API (`/chat`, `/upload`, `/`)                 |
| **Uvicorn**      | ASGI server para rodar a aplicaÃ§Ã£o FastAPI                         |
| **Ollama**       | ExecuÃ§Ã£o local do modelo de linguagem (LLM)                        |
| **Requests**     | ComunicaÃ§Ã£o Python â†’ Ollama (`/api/chat`)                          |
| **SQLite3**      | Banco local para memÃ³ria conversacional persistente                |
| **python-dotenv**| Leitura das variÃ¡veis de ambiente do arquivo `.env`                |
| **PyPDF2**       | Leitura de conteÃºdo de arquivos PDF                               |
| **python-docx**  | Leitura de arquivos `.doc`/`.docx`                                  |
| **HTML + CSS + JS** | Interface web estilo ChatGPT (com animaÃ§Ã£o de digitaÃ§Ã£o, etc.) |
| **Web Speech API** | Reconhecimento de voz (STT) e fala (TTS) no navegador           |

---

## ğŸ“‚ Estrutura do Projeto

> Obs.: nomes e caminhos podem variar ligeiramente conforme sua organizaÃ§Ã£o, mas a ideia geral Ã© essa.

```bash
ia-chat-strands-ollama/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py           # FastAPI: rotas /, /chat, /upload, static, CORS
â”‚   â”œâ”€â”€ agent.py          # LÃ³gica do agente Jasper (Ollama + memÃ³ria + sanitizaÃ§Ã£o)
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ database.py   # FunÃ§Ãµes de init_db, save_message, load_memory (SQLite)
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ index.html    # Interface web principal do Jasper
â”‚       â”œâ”€â”€ style.css     # Tema dark/light, sidebar, bolhas, etc.
â”‚       â””â”€â”€ app.js        # LÃ³gica do chat, animaÃ§Ã£o, voz, upload, etc.
â”œâ”€â”€ .env                  # ConfiguraÃ§Ãµes do modelo e do agente
â”œâ”€â”€ .env.example          # Exemplo de configuraÃ§Ãµes
â”œâ”€â”€ requirements.txt      # DependÃªncias Python
â”œâ”€â”€ INSTALACAO.md         # (Opcional) Guia extra de instalaÃ§Ã£o
â””â”€â”€ README.md             # Este documento
âš™ï¸ ConfiguraÃ§Ã£o e ExecuÃ§Ã£o
1. PrÃ©-requisitos
Python 3.10+

Ollama instalado na mÃ¡quina
ğŸ‘‰ https://ollama.com/

(Opcional, mas recomendado) Git e VS Code

2. Clonar o repositÃ³rio
bash
Copiar cÃ³digo
git clone https://github.com/SEU_USUARIO/ia-chat-strands-ollama.git
cd ia-chat-strands-ollama
3. Criar ambiente virtual e instalar dependÃªncias
bash
Copiar cÃ³digo
# Criar venv
python -m venv .venv

# Ativar venv (Windows PowerShell)
.\.venv\Scripts\activate

# Ativar venv (Linux/macOS)
source .venv/bin/activate

# Instalar dependÃªncias
pip install -r requirements.txt
4. Configurar o Ollama
Inicie o servidor do Ollama:

bash
Copiar cÃ³digo
ollama serve
Baixe o modelo que serÃ¡ usado no .env (ex.: llama3.1):

bash
Copiar cÃ³digo
ollama pull llama3.1
5. Configurar variÃ¡veis de ambiente (.env)
Crie um arquivo .env na raiz do projeto (ao lado de requirements.txt), por exemplo:

ini
Copiar cÃ³digo
AGENT_MODEL="llama3.1"
AGENT_TEMPERATURE="0.2"
AGENT_SYSTEM_PROMPT="VocÃª Ã© Jasper Spencer â€“ Assistente IA Local. Responda SEMPRE de forma natural, clara e humana.

REGRAS IMPORTANTES E OBRIGATÃ“RIAS:
1. NUNCA mostre, mencione, descreva ou revele ferramentas internas como 'calculator'.
2. NUNCA exiba JSON, tool_calls, cÃ³digo, estruturas internas ou metadados.
3. Sempre que houver nÃºmeros, contas ou expressÃµes matemÃ¡ticas, faÃ§a o cÃ¡lculo internamente e responda apenas com o resultado final.
4. A resposta ao usuÃ¡rio deve ser SOMENTE o resultado final â€” humano, natural e direto.
5. Para cÃ¡lculos, responda apenas: 'A resposta Ã© X.' sem mostrar passos.
6. Para perguntas comuns, responda em 1â€“2 frases claras.
7. Se o modelo tentar responder em JSON ou formato tÃ©cnico, ignore isso e devolva apenas texto natural em portuguÃªs."
VocÃª pode ajustar modelo, temperatura e prompt do sistema Ã  vontade.

6. Rodar o backend (FastAPI)
Com a venv ativada e o Ollama rodando, execute:

bash
Copiar cÃ³digo
uvicorn app.main:app --reload --host 127.0.0.1 --port 8001
A API estarÃ¡ acessÃ­vel em:
ğŸ‘‰ http://127.0.0.1:8001

A interface web (frontend) estÃ¡ em:
ğŸ‘‰ http://127.0.0.1:8001/

O projeto pode abrir automaticamente o navegador, dependendo da funÃ§Ã£o abrir_navegador_automaticamente() em main.py.

ğŸŒ Endpoints principais
1ï¸âƒ£ GET /
Retorna a pÃ¡gina HTML com o chat do Jasper (index.html).

Essa pÃ¡gina consome as rotas /chat e /upload via JavaScript (Axios).

2ï¸âƒ£ POST /chat
Endpoint principal do case.

Request (JSON):

json
Copiar cÃ³digo
{
  "message": "Explique o que Ã© a linguagem Lua.",
  "session_id": "opcional-id-da-sessao"
}
session_id Ã© opcional; se nÃ£o fornecido, o backend usa um id padrÃ£o (default ou similar).

Response (JSON):

json
Copiar cÃ³digo
{
  "response": "Lua Ã© uma linguagem de programaÃ§Ã£o leve, interpretada e muito utilizada em jogos e sistemas embarcados pela sua velocidade e facilidade de integraÃ§Ã£o."
}
Fluxo interno:

main.py recebe a requisiÃ§Ã£o, valida com Pydantic (ChatRequest).

Envia a mensagem para run_agent() em agent.py.

agent.py:

Carrega histÃ³rico recente da sessÃ£o via SQLite (load_memory).

Monta o prompt com:

system prompt (AGENT_SYSTEM_PROMPT)

memÃ³ria (Ãºltimas mensagens)

mensagem atual do usuÃ¡rio

Comprime o histÃ³rico se estiver muito grande (controle de contexto).

Chama o Ollama (POST http://localhost:11434/api/chat).

Sanitiza a resposta (remove marcas de JSON, tool_call, etc.).

Salva a mensagem do usuÃ¡rio e a resposta do bot no banco (save_message).

Retorna apenas texto natural em portuguÃªs no campo response.

3ï¸âƒ£ POST /upload
Faz upload de arquivos PDF/DOC/DOCX, extrai o texto e gera um resumo com o agente.

Request (multipart/form-data):

http
Copiar cÃ³digo
POST /upload
Content-Type: multipart/form-data

file: meu-documento.pdf
Response (JSON):

json
Copiar cÃ³digo
{
  "summary": "Resumo objetivo do conteÃºdo do documento..."
}
Fluxo:

main.py recebe o arquivo em UploadFile.

Se for .pdf, usa PyPDF2 para extrair texto.

Se for .doc / .docx, usa python-docx.

Monta um prompt do tipo:
"Resuma o seguinte conteÃºdo de forma clara e objetiva: ..." (limitando tamanho)

Chama run_agent() passando esse texto.

Retorna o resumo no campo summary.

ğŸ§  MemÃ³ria Conversacional (SQLite)
O projeto inclui um mÃ³dulo de memÃ³ria em app/memory/database.py com:

init_db()
Cria o banco jasper_memory.db e a tabela de mensagens, caso nÃ£o existam.

save_message(session_id, role, content)
Salva cada mensagem (usuÃ¡rio ou assistente) com:

session_id

role (user ou assistant)

content

created_at (timestamp)

load_memory(session_id, limit=10)
Carrega as Ãºltimas N mensagens de uma sessÃ£o, para enviar junto ao prompt e gerar contexto.

No agent.py, essas funÃ§Ãµes sÃ£o usadas para:

Construir o histÃ³rico de conversa

Manter o contexto entre as mensagens

Controlar o tamanho mÃ¡ximo do contexto enviado ao modelo (compressÃ£o pelo nÃºmero de caracteres)

ğŸ¨ Frontend â€“ UI estilo ChatGPT/Gemini
Localizado em app/static/:

index.html

style.css

app.js

Principais recursos da UI
Tema dark com gradiente futurista e glassmorphism

Avatar do Jasper (avatar.png) com glow animado

Sidebar com:

Logo + nome do assistente

BotÃ£o "Nova conversa" (limpa o histÃ³rico local)

SeÃ§Ã£o de â€œHistÃ³ricoâ€ preparada para expansÃ£o futura

Ãrea de "ConfiguraÃ§Ãµes" / "Sobre" (placeholder para futuras features)

Ãrea central de chat com:

SugestÃµes iniciais de prompts

Bolhas de mensagem do usuÃ¡rio (direita, gradiente azul) e do bot (esquerda, vidro escuro)

AnimaÃ§Ã£o de digitaÃ§Ã£o para respostas do bot (texto aparecendo aos poucos)

Indicador "Jasper estÃ¡ digitando..." com 3 bolinhas animadas

RodapÃ© com:

BotÃ£o ğŸ“ para upload de arquivos (PDF/DOCX)

BotÃ£o ğŸ¤ para modo voz (fala com o Jasper)

Campo de texto

BotÃ£o de envio (Ã­cone de aviÃ£o de papel)

BotÃ£o ğŸ—‘ para limpar o chat (limpa localStorage e recarrega sugestÃµes iniciais)

Suporte a tema claro/escuro, persistido via localStorage

ğŸ”Š Modo Voz (STT + TTS)
Implementado no app.js usando APIs nativas do navegador:

TTS (Text-to-Speech)
Usa window.speechSynthesis para o Jasper falar as respostas (pode ser acionado onde vocÃª desejar).

STT (Speech-to-Text)
Usa SpeechRecognition/webkitSpeechRecognition para:

Ouvir o microfone

Converter fala em texto

Enviar para o backend automaticamente

âš  O suporte depende do navegador (Chrome/Edge geralmente OK).

ğŸ“¦ Como este projeto atende ao case
Requisitos principais do case: 
Teste - EstÃ¡gio IA


ConfiguraÃ§Ã£o do Ambiente

âœ… Uso de .env para configurar modelo, sistem prompt e temperatura.

âœ… requirements.txt com todas as dependÃªncias (FastAPI, dotenv, etc.).

ImplementaÃ§Ã£o da API (FastAPI)

âœ… Endpoint POST /chat que recebe { "message": "..." } e retorna { "response": "..." }.

âœ… Servidor rodando com uvicorn conforme boas prÃ¡ticas.

Agente de IA

âœ… ImplementaÃ§Ã£o de um agente central em agent.py.

âœ… IntegraÃ§Ã£o com LLM local via Ollama.

âœ… Capacidade de responder perguntas gerais.

âœ… Capacidade de lidar com cÃ¡lculos numÃ©ricos via instruÃ§Ãµes no AGENT_SYSTEM_PROMPT (o modelo faz os cÃ¡lculos de forma segura e direta).

Extras alÃ©m do case

ğŸŒŸ MemÃ³ria conversacional persistente com SQLite.

ğŸŒŸ UI rica estilo ChatGPT/Gemini.

ğŸŒŸ Upload e leitura de documentos (PDF/DOCX) com resumo automÃ¡tico.

ğŸŒŸ Modo de voz (voz â†’ texto â†’ IA â†’ texto â†’ voz).

ğŸŒŸ Estrutura preparada para expansÃ£o (novas ferramentas, histÃ³rico de conversas, etc.).

ğŸ§ª Testes rÃ¡pidos
Swagger (se exposto em /docs, opcional)
http://127.0.0.1:8001/docs

Testar chat via curl:

bash
Copiar cÃ³digo
curl -X POST "http://127.0.0.1:8001/chat" \
  -H "Content-Type: application/json" \
  -d "{\"message\": \"Quanto Ã© 123 * 45?\"}"
Testar upload (via frontend)
Acesse http://127.0.0.1:8001, clique no Ã­cone de clipe ğŸ“ e envie um PDF/DOCX.

ğŸš€ PrÃ³ximos passos / ideias de evoluÃ§Ã£o
Implementar histÃ³rico real de conversas na sidebar (frontend + backend).

Adicionar autenticaÃ§Ã£o simples/token para uso em rede interna.

Containerizar com Docker para rodar backend + Ollama em um ambiente padronizado.

Adaptar o agente para usar um sistema de â€œferramentasâ€ mais formal (ex: integrar novamente com Strands Agents ou outro orchestrator).

Deploy em VPS, expondo o Jasper como assistente privado na nuvem.

Se vocÃª estiver usando este projeto como case de estÃ¡gio, este README jÃ¡:

Explica o contexto (case da empresa)

Mostra arquitetura e decisÃµes tÃ©cnicas

Demonstra que vocÃª foi alÃ©m do mÃ­nimo (memÃ³ria, UI, voz, uploads)

Facilita a vida de qualquer avaliador para rodar tudo localmente ğŸ’œ#   i a _ j a s p e r _ s p e n c e r - c h a t - s t r a n d s - o l l a m a _  
 