üìñ Guia de Instala√ß√£o e Execu√ß√£o ‚Äì Jasper Spencer (IA Local com Ollama)

Este guia mostra, passo a passo, como baixar, instalar e rodar o projeto na sua m√°quina, desde zero at√© abrir o chat no navegador.

‚úÖ Pr√©-requisitos

Antes de tudo, voc√™ precisa ter instalado:

Python 3.10 ou superior
‚ûú Download: https://www.python.org/downloads/

Ollama (motor da IA local)
‚ûú Download: https://ollama.com/

üí° Sistema operacional: funciona em Windows, macOS ou Linux (desde que o Ollama seja suportado).

1Ô∏è‚É£ Baixar o projeto

No terminal (PowerShell, cmd ou Linux/macOS):

git clone https://github.com/SEU_USUARIO/ia-chat-strands-ollama.git
cd ia-chat-strands-ollama


Troque SEU_USUARIO pelo seu usu√°rio do GitHub.

2Ô∏è‚É£ Criar e ativar o ambiente virtual (venv)
Windows (PowerShell)
python -m venv .venv
.\.venv\Scripts\Activate.ps1

Windows (Prompt de Comando ‚Äì cmd)
python -m venv .venv
.\.venv\Scripts\activate

Linux / macOS
python3 -m venv .venv
source .venv/bin/activate


Voc√™ saber√° que deu certo quando aparecer algo como (.venv) no come√ßo da linha do terminal.

3Ô∏è‚É£ Instalar as depend√™ncias Python

Com a venv ativada:

pip install -r requirements.txt


Caso o projeto n√£o tenha ainda no requirements.txt, instale tamb√©m as libs usadas no upload de arquivos:

pip install PyPDF2 python-docx

4Ô∏è‚É£ Instalar e configurar o Ollama
4.1 Instalar

Siga o instalador do site:
https://ollama.com/

4.2 Baixar o modelo da IA

No terminal:

ollama pull llama3.1


Se quiser usar outro modelo (ex.: phi3, qwen2.5, etc.), √© s√≥ mudar o nome depois no .env.

4.3 Deixar o servidor do Ollama ligado

Abra um novo terminal e rode:

ollama serve


Deixe esse terminal aberto.
Ele √© o ‚Äúmotor‚Äù que o Jasper usa por tr√°s.

5Ô∏è‚É£ Configurar o arquivo .env

Na pasta do projeto (ia-chat-strands-ollama), crie um arquivo chamado .env (se ainda n√£o tiver) com algo assim:

AGENT_MODEL="llama3.1"
AGENT_TEMPERATURE="0.2"
AGENT_SYSTEM_PROMPT="Voc√™ √© Jasper Spencer ‚Äì Assistente IA Local. Responda SEMPRE de forma natural, clara e humana.

REGRAS IMPORTANTES E OBRIGAT√ìRIAS:
1. NUNCA mostre, mencione, descreva ou revele ferramentas internas.
2. NUNCA exiba JSON, tool_calls, c√≥digo, estruturas internas ou metadados.
3. Sempre que houver n√∫meros, contas ou express√µes matem√°ticas, fa√ßa o c√°lculo internamente e responda apenas com o resultado final.
4. A resposta ao usu√°rio deve ser SOMENTE o resultado final ‚Äî humano, natural e direto.
5. Para c√°lculos, responda apenas: 'A resposta √© X.' sem mostrar passos.
6. Para perguntas comuns, responda em 1‚Äì2 frases claras.
7. Se o modelo tentar responder em JSON ou formato t√©cnico, ignore isso e devolva apenas texto natural em portugu√™s."


AGENT_MODEL ‚Üí nome do modelo do Ollama (tem que ser o mesmo que voc√™ deu ollama pull).

AGENT_TEMPERATURE ‚Üí 0.2 deixa o modelo mais est√°vel.

AGENT_SYSTEM_PROMPT ‚Üí personalidade e regras do Jasper.

6Ô∏è‚É£ Rodar o backend (FastAPI)

Volte no terminal onde a venv est√° ativada (.venv) e rode:

uvicorn app.main:app --reload --host 127.0.0.1 --port 8001


Se tudo estiver ok, voc√™ ver√° algo como:

Uvicorn running on http://127.0.0.1:8001
Application startup complete.

7Ô∏è‚É£ Abrir o chat no navegador

Abra seu navegador (Chrome, Edge, etc.) e acesse:

http://127.0.0.1:8001/


Voc√™ ver√° a interface do Jasper Spencer com:

Sidebar

Avatar

Campo de mensagem

Bot√µes de upload, voz, limpar, etc.

Agora √© s√≥ conversar com a IA üòÑ

8Ô∏è‚É£ Testando o endpoint /chat (opcional)

Se quiser testar s√≥ a API, sem o frontend:

Via curl:
curl -X POST "http://127.0.0.1:8001/chat" ^
  -H "Content-Type: application/json" ^
  -d "{\"message\": \"Me explique rapidamente o que √© FastAPI.\"}"


(Em Linux/macOS, tire o ^ e deixe em uma linha s√≥ ou use \).

Resposta esperada (exemplo):

{
  "response": "FastAPI √© um framework web em Python focado em alta performance e constru√ß√£o r√°pida de APIs."
}

9Ô∏è‚É£ Envio de arquivos (PDF/DOCX) ‚Äì via interface

Pela pr√≥pria tela do Jasper:

Clique no √≠cone de clipe üìé

Escolha um arquivo .pdf, .doc ou .docx

O backend vai:

ler o arquivo

extrair o texto

pedir ao Jasper um resumo

A resposta aparecer√° como uma mensagem do bot.

üîö Recap r√°pido

Clonar o projeto

Criar e ativar .venv

pip install -r requirements.txt

Instalar Ollama + ollama pull llama3.1 + ollama serve

Criar .env com AGENT_MODEL, AGENT_TEMPERATURE, AGENT_SYSTEM_PROMPT

uvicorn app.main:app --reload --host 127.0.0.1 --port 8001

Abrir http://127.0.0.1:8001/ no navegador